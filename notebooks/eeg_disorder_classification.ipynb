{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-Based Neurological Disorder Classification\n",
    "## Rice Datathon 2025 - Neurotech Track\n",
    "\n",
    "This notebook implements machine learning models to classify neurological disorders using EEG data.\n",
    "\n",
    "### Models Implemented:\n",
    "- **XGBoost** (Primary model)\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n",
    "- Logistic Regression (One-vs-Rest approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to generate competition predictions\n",
    "# Set to False for model validation and testing\n",
    "PRODUCTION_MODE = False\n",
    "\n",
    "# File paths\n",
    "TRAIN_DATA_PATH = '../data/Train_and_Validate_EEG.csv'\n",
    "TEST_DATA_PATH = '../data/Test_Set_EEG.csv'\n",
    "RESULTS_PATH = '../results/'\n",
    "\n",
    "# Model parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CORRELATION_THRESHOLD = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget classes: {train_df['main.disorder'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "train_df = train_df.iloc[:, :123]\n",
    "test_df = test_df.iloc[:, :120]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['specific.disorder', 'ID', 'eeg.date']\n",
    "train_df = train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns])\n",
    "\n",
    "# Store test IDs for final submission\n",
    "test_ids = test_df[['ID']].copy()\n",
    "test_df = test_df.drop(columns=['eeg.date', 'ID'], errors='ignore')\n",
    "\n",
    "print(f\"Shape after initial cleaning - Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values in training data\n",
    "missing_counts = train_df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_counts) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_counts.head(20).plot(kind='bar', color='coral', edgecolor='black')\n",
    "    plt.title('Top 20 Columns with Missing Values', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Column Names')\n",
    "    plt.ylabel('Number of Missing Values')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Handle missing values in test data\n",
    "test_df['IQ'] = test_df['IQ'].fillna(test_df['IQ'].mean())\n",
    "test_df['education'] = test_df['education'].fillna(test_df['education'].mean())\n",
    "\n",
    "# Remove rows with missing values from training data\n",
    "train_df_clean = train_df.drop(columns=['Unnamed: 122'], errors='ignore').dropna()\n",
    "print(f\"\\nRows removed due to missing values: {len(train_df) - len(train_df_clean)}\")\n",
    "print(f\"Final training data shape: {train_df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Target Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "target_distribution = train_df_clean['main.disorder'].value_counts()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "target_distribution.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Distribution of Neurological Disorders', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Disorder Type')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(target_distribution)))\n",
    "ax2.pie(target_distribution, labels=target_distribution.index, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "ax2.set_title('Disorder Distribution (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(target_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Selection - Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "numerical_features = train_df_clean.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numerical_features.corr().abs()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Identify features to drop\n",
    "features_to_drop = [column for column in upper_triangle.columns \n",
    "                   if any(upper_triangle[column] > CORRELATION_THRESHOLD)]\n",
    "\n",
    "print(f\"Removing {len(features_to_drop)} highly correlated features (correlation > {CORRELATION_THRESHOLD})\")\n",
    "print(f\"Features removed: {features_to_drop[:5]}...\" if len(features_to_drop) > 5 else f\"Features removed: {features_to_drop}\")\n",
    "\n",
    "# Drop highly correlated features\n",
    "train_df_clean = train_df_clean.drop(columns=features_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"\\nShape after removing correlated features - Train: {train_df_clean.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sex variable\n",
    "label_encoder_sex = LabelEncoder()\n",
    "train_df_clean['sex'] = label_encoder_sex.fit_transform(train_df_clean['sex'])\n",
    "test_df['sex'] = label_encoder_sex.transform(test_df['sex'])\n",
    "\n",
    "print(\"Categorical encoding completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train_df_clean.drop(columns=['main.disorder'])\n",
    "y = train_df_clean['main.disorder']\n",
    "\n",
    "# Create train-test split or use full data for production\n",
    "if PRODUCTION_MODE:\n",
    "    X_train, X_test = X, test_df\n",
    "    y_train, y_test = y, None\n",
    "    print(\"Production mode: Using full training data\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"Validation mode: Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 XGBoost Model (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "if PRODUCTION_MODE:\n",
    "    # Save predictions for submission\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': test_ids['ID'],\n",
    "        'main.disorder.class': label_encoder.inverse_transform(y_pred_xgb)\n",
    "    })\n",
    "    predictions_df.to_csv(f'{RESULTS_PATH}xgb_predictions.csv', index=False)\n",
    "    print(f\"XGBoost predictions saved to {RESULTS_PATH}xgb_predictions.csv\")\n",
    "else:\n",
    "    # Evaluate model\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "    print(f\"\\nXGBoost Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        label_encoder.inverse_transform(y_test_encoded),\n",
    "        label_encoder.inverse_transform(y_pred_xgb)\n",
    "    ))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        label_encoder.inverse_transform(y_test_encoded),\n",
    "        label_encoder.inverse_transform(y_pred_xgb),\n",
    "        xticks_rotation='vertical',\n",
    "        cmap='Blues'\n",
    "    )\n",
    "    disp.ax_.set_title(\"XGBoost Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (XGBoost)', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training SVM model...\")\n",
    "\n",
    "# Initialize and train SVM\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "if PRODUCTION_MODE:\n",
    "    # Save predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': test_ids['ID'],\n",
    "        'main.disorder.class': y_pred_svm\n",
    "    })\n",
    "    predictions_df.to_csv(f'{RESULTS_PATH}svm_predictions.csv', index=False)\n",
    "    print(f\"SVM predictions saved to {RESULTS_PATH}svm_predictions.csv\")\n",
    "else:\n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    print(f\"\\nSVM Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Initialize and train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "if PRODUCTION_MODE:\n",
    "    # Save predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': test_ids['ID'],\n",
    "        'main.disorder.class': label_encoder.inverse_transform(y_pred_rf)\n",
    "    })\n",
    "    predictions_df.to_csv(f'{RESULTS_PATH}rf_predictions.csv', index=False)\n",
    "    print(f\"Random Forest predictions saved to {RESULTS_PATH}rf_predictions.csv\")\n",
    "else:\n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred_rf)\n",
    "    print(f\"\\nRandom Forest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        label_encoder.inverse_transform(y_test_encoded),\n",
    "        label_encoder.inverse_transform(y_pred_rf)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Binary Classification Analysis (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRODUCTION_MODE:\n",
    "    print(\"\\nPerforming binary classification analysis (disorder vs healthy control)...\\n\")\n",
    "    \n",
    "    disorders = ['Addictive disorder', 'Anxiety disorder', 'Mood disorder', \n",
    "                'Obsessive compulsive disorder', 'Schizophrenia', \n",
    "                'Trauma and stress related disorder']\n",
    "    \n",
    "    binary_results = []\n",
    "    \n",
    "    for disorder in disorders:\n",
    "        # Filter data for binary classification\n",
    "        binary_mask_train = y_train.isin([disorder, 'Healthy control'])\n",
    "        binary_mask_test = y_test.isin([disorder, 'Healthy control'])\n",
    "        \n",
    "        X_train_binary = X_train_scaled[binary_mask_train]\n",
    "        y_train_binary = y_train[binary_mask_train]\n",
    "        X_test_binary = X_test_scaled[binary_mask_test]\n",
    "        y_test_binary = y_test[binary_mask_test]\n",
    "        \n",
    "        # Encode binary labels\n",
    "        le_binary = LabelEncoder()\n",
    "        y_train_binary_encoded = le_binary.fit_transform(y_train_binary)\n",
    "        y_test_binary_encoded = le_binary.transform(y_test_binary)\n",
    "        \n",
    "        # Train logistic regression\n",
    "        lr_model = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=1.0,\n",
    "            max_iter=1000,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        lr_model.fit(X_train_binary, y_train_binary_encoded)\n",
    "        y_pred_binary = lr_model.predict(X_test_binary)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test_binary_encoded, y_pred_binary)\n",
    "        binary_results.append({\n",
    "            'Disorder': disorder,\n",
    "            'Accuracy': accuracy,\n",
    "            'Test Samples': len(y_test_binary)\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(binary_results)\n",
    "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(results_df['Disorder'], results_df['Accuracy'], color='teal')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Binary Classification Performance (Disorder vs Healthy Control)', fontweight='bold')\n",
    "    plt.xlim([0, 1])\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for i, (disorder, acc) in enumerate(zip(results_df['Disorder'], results_df['Accuracy'])):\n",
    "        plt.text(acc + 0.01, i, f'{acc:.3f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBinary Classification Results:\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRODUCTION_MODE:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nBest performing model: XGBoost\")\n",
    "    print(\"\\nRecommendation: Use XGBoost predictions for final submission\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PRODUCTION RUN COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nPrediction files saved to: {RESULTS_PATH}\")\n",
    "    print(\"- xgb_predictions.csv (RECOMMENDED)\")\n",
    "    print(\"- svm_predictions.csv\")\n",
    "    print(\"- rf_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}